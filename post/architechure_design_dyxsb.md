+++
date = "2017-03-05"
title = "小书包系统架构的设计"
draft = false
description = ""
tags        = ["架构"]
categories = [ "技术"]
+++

小书包的架构历史及发展方向

<!--more-->

# 开始 V1
系统包括后台服务, 后台管理系统, 两个终端应用( IOS 和 安卓). 其中, 后台服务包含系统功能模型, 在线聊天; 后台管理系统用于后台管理员录入系统数据. 终端 App 访问后台服务获取数据, 并且向后台服务发送数据.

系统功能模型还是比较复杂的, 不像一般的互联网产品, 而且需求尚在变动中, 上线前修改了好多次. 


写下第一行代码的时候, 也是小书包背负最多紧急任务的时候. 在这之前已经开发了一个版本, 我们接手维护, 但是面对需求, 这个版本没有办法完成任务. 原因包括几个:
- 之前的版本基于 Java 技术栈, 现在的研发没有这方面的经验, 而且剩余的时间也不够转型
- 之前的代码在交接时缺少关键部分, 没法运行
- 经过评估, 在原来代码的基础上进行修补和改进, 成本比重新开发还要高

决定重新开发后, 选择了 Go 作为后台主要开发语言, 原因也有几个:
- 我自己对Go 开发有很长时间的经验, 遇到问题能够自行解决
- 语言简单, 有经验的程序员上手非常快
- Go 部署简单明确
- Go 的运行效率也够高

确定采用的技术后, 面临的需求如下:
- 开发要快, 但是功能模型还要超越之前的版本, 只有大约三个月的时间
- 系统效率要足够, 可能有几十万人同时在线

为了满足这两个需求, 系统第一版采用了内存模型, 不仅功能模型保存在内存中, 保证了开发速度, 同时内存作为缓存满足性能的要求. 系统如期上线, 基本没出什么大问题, 为了满足下一轮的用户量增长的需求, 需要对系统架构进行升级.

# 主服务

## 静态文件 V2

V1版本将所有的请求揽于一身, 包括静态资源的处理. 

### 1. 网页静态资源文件
静态文件分离的第一步就是将网页所需的静态资源简单的分离出来, 以方便对静态文件的请求进行单独处理. 通过将静态文件存放在单独的服务器上, 并通过 nginx 作为文件服务器后, 完成了初步的改造, 这样做的目的有两个:
1. 网页请求不再有主服务处理
2. 对静态文件服务进行测试, 为接下来的分离做测试

### 2. 静态数据文件

第一步升级完成后, 系统本身的静态文件的处理完成, 但是系统运行本身也包含很多的静态文件数据, 这些相对来说复杂了一些. 举例来说, 系统需要上传图书文件, 而终端则会大量的同时下载(业务本身需求导致). 如果并发下载量超过一定量, 同样会影响系统的稳定.

之前上传图书文件由系统本身处理, 流程相对简单. 为了将图书文件作为静态文件单独处理, 新的流程将上传图书的流程分为两步:
1. 将图书文件本身上传到静态文件服务器
2. 将上传成功的标记作为参数提交到系统主服务

为了完成第一个流程, 需要创建新的上传服务, 该服务接收文件的上传, 将文件处理后交由文件服务器提供文件下载服务, 并返回文件路径信息. 该文件路径信息之后会被提交到主服务, 并由主服务分发给终端 App, 终端 App 最终能够获取到最新正确的文件地址, 从而整个服务流程完成. 

完成了图书文件的上传流程的改造后, 我们用到的上传服务进行简单的扩展, 就可以将包括图书封面, 文章阅读等最终以静态文件存在的流程全部进行改造, 包括在线聊天这个对文件上传需求量最高的系统服务, 这样的结果就是:
- 系统主服务的负担进一步减轻
- 类似服务统一处理, 专门优化
- 使用上传服务的新功能的开发复杂度降低


## 系统模块化 V3

尽管静态文件相关的服务被剥离出来, 但是系统主服务还是太过庞大, 造成的影响包括:
- 所有功能模块的集中, 导致开发修改任何功能都是对主服务的改动, 任何的微小 bug 都会导致主服务受影响
- 对于新成员, 无论开发功能还是修改 bug, 设计的代码范围大, 掌握系统设计的速度也比较慢
- 由于所有的资源都在同一个区域内, 功能开发很容易形成依赖, 如果开发成员的模块化意识差, 很容易写出混乱的代码, 给维护带来较大麻烦

为了解决这些问题, V3版本的任务是逐渐的在主服务中将功能进行拆分, 最终形成独立的功能服务.

### 大前后端的分离

一直以来, 后台管理系统与后台的数据服务是集合在主服务中的, 这样导致的问题就是:
- 即使业务需求导致的页面变化非常微小, 也需要对主服务进行升级
- 前端开发需要搭建和主服务同样的系统才能进行开发, 增大了开发难度

为了解决这两个问题, 对系统主服务中的前端部分拆分成单独的前端系统, 通过 API 的方式调用数据

### 功能模块的拆分

功能服务独立化时面对的问题主要是:
- 如何确定功能的边界
- 之前同一个服务进程内的依赖问题如何解决
- 模块之间的数据如何同步

功能边界的问题主要在于如何定义业务, 在这方面经过了较长时间的业务运营, 逐渐总结出了业务的特点, 因此渐渐划清了模块的大体边界, 但是这个边界不是固定的, 需要根据业务的变化及时调整. 

将原来的一个服务切分, 原来内存中查找数据就可以完成的操作, 现在划分为多个流程的组合, 很多问题就会产生, 比如说中间出错怎么处理, 性能降低较大怎么办? 这方面有放多技术上的解决方案, 比如 内部 Restful API, RPC 等等. 如果解决这个问题只是用这些技术方案代替原来的内存查找, 我觉得有很大的浪费. 因为模块的划分, 本来就是从底层逼迫架构设计者考虑之前数据调用的合理性, 如果能够通过模块的划分, 将原来隐藏的问题暴露出来, 通过重新思考, 架构方案的改进对整个系统加以改进, 那对于系统的清晰和可维护性一定是个巨大的进步. 

对于模块独立后, 对于必须依赖的数据的同步, 我们采用的方案是中间件事件提醒, 然后从数据库查询同步的方案. 具体来说, 每一个模块都有自己的缓存数据, 每个模块都可能具有读和写的操作, 如果发生写操作, 那么在数据成功写入到数据库完成持久化以后, 需要发送一个对该数据发生变化的事件提醒到中间件, 依赖该数据的模块通过订阅该事件获得提醒, 之后通过查询数据库中的相关数据, 更新缓存数据.

系统模块化之后, 不仅解决了之前存在的问题, 还额外得到了另外一些好处:
- 由于模块之间的数据依赖, 促使系统不断的向无状态, 内聚的方向进化, 系统之间的耦合性降低, 整个系统更加清晰, 为之后功能的开发提供了坚实的基础
- 由于每个模块负责管理自己的数据, 但整个系统对外提供的 API 需要提供跨模块的数据, 促使缓存层的单独设计, 同时将终端和后台的 API 进行了分类处理

## 系统的升级 V4

好长时间以来, 系统的升级维护工作都需要暂停服务, 暂停服务需要经历一系列的流程, 流程的繁杂会阻碍功能的及时升级, 这样会造成很多问题:
- 功能不能快速上线, 会造成功能的堆积, 堆积增多与升级出错的概率成正比, 导致系统升级出现意外的可能性增大
- 功能不能及时上线, 功能的反馈及时性降低, 不能及时跟进客户真正需求, 时间成本增大
- 功能不能上线, 依据该功能的后续开发工作不确定性增大, 开发成本升高
- 体验差

造成这个问题的主要原因在于系统的缓存使用的是内存. 为了解决这个问题, 将系统模块中的缓存用 redis 替代, 通过共享缓存, 可以同时启动同一模块的多个实例, 通过负载均衡的配置, 可以逐步用升级版本的实例代替就实例, 实现无缝升级. 另外一个好处就是, 再将模块的读写操作进行分离后, 就顺带实现了系统的横向扩展, 系统的承载量不再是问题

## 向数据平台的演进 V5

经过之前的改进, 整个系统在业务上基本不存在大的问题. 随着系统的增大, 和第三方资源的对接成为主要考虑的方向, 不仅包括接入第三方的数据资源, 也包括开发资源, 如何向第三方开放数据接口, 增大系统的功能扩展能力摆在面前. 

解决思路是整个系统向平台化演进. 将内部的各个模块在现有的业务层和数据层的基础上, 从纵向上进一步细化拆分, 业务层分为基础业务和组合业务, 数据层分为业务数据和基础数据, 每个层次上开放合适的 API服务, 第三方的开发能力在服务基础上, 就可以快速得以实现. 